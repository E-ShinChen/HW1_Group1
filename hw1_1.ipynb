{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\mb207\\Anaconda3\\envs\\opencv\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, precision_score, recall_score, f1_score\n",
    "from keras import layers, optimizers, models\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import Image, SVG\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv(\"Shill Bidding Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record_ID</th>\n",
       "      <th>Auction_ID</th>\n",
       "      <th>Bidder_ID</th>\n",
       "      <th>Bidder_Tendency</th>\n",
       "      <th>Bidding_Ratio</th>\n",
       "      <th>Successive_Outbidding</th>\n",
       "      <th>Last_Bidding</th>\n",
       "      <th>Auction_Bids</th>\n",
       "      <th>Starting_Price_Average</th>\n",
       "      <th>Early_Bidding</th>\n",
       "      <th>Winning_Ratio</th>\n",
       "      <th>Auction_Duration</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>732</td>\n",
       "      <td>_***i</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>732</td>\n",
       "      <td>g***r</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>732</td>\n",
       "      <td>t***p</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>732</td>\n",
       "      <td>7***n</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>900</td>\n",
       "      <td>z***z</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316</th>\n",
       "      <td>15129</td>\n",
       "      <td>760</td>\n",
       "      <td>l***t</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.738557</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.686358</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>15137</td>\n",
       "      <td>2481</td>\n",
       "      <td>s***s</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6318</th>\n",
       "      <td>15138</td>\n",
       "      <td>2481</td>\n",
       "      <td>h***t</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015663</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.015663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6319</th>\n",
       "      <td>15139</td>\n",
       "      <td>2481</td>\n",
       "      <td>d***d</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068694</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6320</th>\n",
       "      <td>15144</td>\n",
       "      <td>2481</td>\n",
       "      <td>a***l</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.340351</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.340351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6321 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Record_ID  Auction_ID Bidder_ID  Bidder_Tendency  Bidding_Ratio  \\\n",
       "0             1         732     _***i         0.200000       0.400000   \n",
       "1             2         732     g***r         0.024390       0.200000   \n",
       "2             3         732     t***p         0.142857       0.200000   \n",
       "3             4         732     7***n         0.100000       0.200000   \n",
       "4             5         900     z***z         0.051282       0.222222   \n",
       "...         ...         ...       ...              ...            ...   \n",
       "6316      15129         760     l***t         0.333333       0.160000   \n",
       "6317      15137        2481     s***s         0.030612       0.130435   \n",
       "6318      15138        2481     h***t         0.055556       0.043478   \n",
       "6319      15139        2481     d***d         0.076923       0.086957   \n",
       "6320      15144        2481     a***l         0.016393       0.043478   \n",
       "\n",
       "      Successive_Outbidding  Last_Bidding  Auction_Bids  \\\n",
       "0                       0.0      0.000028      0.000000   \n",
       "1                       0.0      0.013123      0.000000   \n",
       "2                       0.0      0.003042      0.000000   \n",
       "3                       0.0      0.097477      0.000000   \n",
       "4                       0.0      0.001318      0.000000   \n",
       "...                     ...           ...           ...   \n",
       "6316                    1.0      0.738557      0.280000   \n",
       "6317                    0.0      0.005754      0.217391   \n",
       "6318                    0.0      0.015663      0.217391   \n",
       "6319                    0.0      0.068694      0.217391   \n",
       "6320                    0.0      0.340351      0.217391   \n",
       "\n",
       "      Starting_Price_Average  Early_Bidding  Winning_Ratio  Auction_Duration  \\\n",
       "0                   0.993593       0.000028       0.666667                 5   \n",
       "1                   0.993593       0.013123       0.944444                 5   \n",
       "2                   0.993593       0.003042       1.000000                 5   \n",
       "3                   0.993593       0.097477       1.000000                 5   \n",
       "4                   0.000000       0.001242       0.500000                 7   \n",
       "...                      ...            ...            ...               ...   \n",
       "6316                0.993593       0.686358       0.888889                 3   \n",
       "6317                0.993593       0.000010       0.878788                 7   \n",
       "6318                0.993593       0.015663       0.000000                 7   \n",
       "6319                0.993593       0.000415       0.000000                 7   \n",
       "6320                0.993593       0.340351       0.000000                 7   \n",
       "\n",
       "      Class  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "6316      1  \n",
       "6317      0  \n",
       "6318      0  \n",
       "6319      0  \n",
       "6320      0  \n",
       "\n",
       "[6321 rows x 13 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bidder_Tendency</th>\n",
       "      <th>Bidding_Ratio</th>\n",
       "      <th>Successive_Outbidding</th>\n",
       "      <th>Last_Bidding</th>\n",
       "      <th>Auction_Bids</th>\n",
       "      <th>Starting_Price_Average</th>\n",
       "      <th>Early_Bidding</th>\n",
       "      <th>Winning_Ratio</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016844</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006781</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006774</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.768044</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016311</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.121951</td>\n",
       "      <td>0.185185</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.035021</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.993528</td>\n",
       "      <td>0.023963</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.570994</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.413788</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bidder_Tendency  Bidding_Ratio  Successive_Outbidding  Last_Bidding  \\\n",
       "0         0.200000       0.400000                    0.0      0.000028   \n",
       "1         0.024390       0.200000                    0.0      0.013123   \n",
       "2         0.142857       0.200000                    0.0      0.003042   \n",
       "3         0.100000       0.200000                    0.0      0.097477   \n",
       "4         0.051282       0.222222                    0.0      0.001318   \n",
       "5         0.038462       0.111111                    0.0      0.016844   \n",
       "6         0.400000       0.222222                    0.0      0.006781   \n",
       "7         0.137931       0.444444                    1.0      0.768044   \n",
       "8         0.121951       0.185185                    1.0      0.035021   \n",
       "9         0.155172       0.346154                    0.5      0.570994   \n",
       "\n",
       "   Auction_Bids  Starting_Price_Average  Early_Bidding  Winning_Ratio  Class  \n",
       "0      0.000000                0.993593       0.000028       0.666667      0  \n",
       "1      0.000000                0.993593       0.013123       0.944444      0  \n",
       "2      0.000000                0.993593       0.003042       1.000000      0  \n",
       "3      0.000000                0.993593       0.097477       1.000000      0  \n",
       "4      0.000000                0.000000       0.001242       0.500000      0  \n",
       "5      0.000000                0.000000       0.016844       0.800000      0  \n",
       "6      0.000000                0.000000       0.006774       0.750000      0  \n",
       "7      0.000000                0.000000       0.016311       1.000000      1  \n",
       "8      0.333333                0.993528       0.023963       0.944444      1  \n",
       "9      0.307692                0.993593       0.413788       0.611111      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['Record_ID', 'Auction_ID', 'Bidder_ID', 'Auction_Duration'], axis = 1)\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Class']\n",
    "X = data.drop(['Class'],axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bidder_Tendency</th>\n",
       "      <th>Bidding_Ratio</th>\n",
       "      <th>Successive_Outbidding</th>\n",
       "      <th>Last_Bidding</th>\n",
       "      <th>Auction_Bids</th>\n",
       "      <th>Starting_Price_Average</th>\n",
       "      <th>Early_Bidding</th>\n",
       "      <th>Winning_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6177</th>\n",
       "      <td>0.073171</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.576319</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773</th>\n",
       "      <td>0.009063</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.113917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039645</td>\n",
       "      <td>0.848214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3963</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.247467</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.247467</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2962</th>\n",
       "      <td>0.348837</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.981964</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.981736</td>\n",
       "      <td>0.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1913</th>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057568</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.205882</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.953870</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.950463</td>\n",
       "      <td>0.885714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3398</th>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.877025</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.877025</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.989729</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.989714</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5754</th>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.858646</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.858646</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>0.006803</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.318183</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5056 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bidder_Tendency  Bidding_Ratio  Successive_Outbidding  Last_Bidding  \\\n",
       "6177         0.073171       0.076923                    0.0      0.576319   \n",
       "773          0.009063       0.375000                    0.0      0.113917   \n",
       "3963         0.058824       0.200000                    0.0      0.247467   \n",
       "2962         0.348837       0.666667                    1.0      0.981964   \n",
       "1913         0.026316       0.090909                    0.0      0.057568   \n",
       "...               ...            ...                    ...           ...   \n",
       "896          0.086420       0.205882                    1.0      0.953870   \n",
       "3398         0.180000       0.375000                    1.0      0.877025   \n",
       "410          0.095238       0.058824                    0.0      0.989729   \n",
       "5754         0.090909       0.066667                    0.0      0.858646   \n",
       "4857         0.006803       0.076923                    0.0      0.318183   \n",
       "\n",
       "      Auction_Bids  Starting_Price_Average  Early_Bidding  Winning_Ratio  \n",
       "6177      0.538462                0.993593       0.000033       0.000000  \n",
       "773       0.000000                0.000000       0.039645       0.848214  \n",
       "3963      0.000000                0.000000       0.247467       0.600000  \n",
       "2962      0.600000                0.993593       0.981736       0.857143  \n",
       "1913      0.000000                0.000000       0.057568       0.000000  \n",
       "...            ...                     ...            ...            ...  \n",
       "896       0.470588                0.993593       0.950463       0.885714  \n",
       "3398      0.250000                0.993593       0.877025       1.000000  \n",
       "410       0.470588                0.993593       0.989714       0.000000  \n",
       "5754      0.000000                0.000000       0.858646       0.000000  \n",
       "4857      0.000000                0.000000       0.318183       0.000000  \n",
       "\n",
       "[5056 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bidder_Tendency</th>\n",
       "      <th>Bidding_Ratio</th>\n",
       "      <th>Successive_Outbidding</th>\n",
       "      <th>Last_Bidding</th>\n",
       "      <th>Auction_Bids</th>\n",
       "      <th>Starting_Price_Average</th>\n",
       "      <th>Early_Bidding</th>\n",
       "      <th>Winning_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3830</th>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.882353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2653</th>\n",
       "      <td>0.010204</td>\n",
       "      <td>0.023256</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.581395</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5693</th>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.145825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5282</th>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.532688</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.628854</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.108468</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.616516</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.616481</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4958</th>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.552847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.552847</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.768054</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.999935</td>\n",
       "      <td>0.768054</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.410655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410655</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1265 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bidder_Tendency  Bidding_Ratio  Successive_Outbidding  Last_Bidding  \\\n",
       "3830         0.086957       0.285714                    0.0      0.002994   \n",
       "2653         0.010204       0.023256                    0.0      0.000009   \n",
       "5693         0.043478       0.400000                    0.0      0.145825   \n",
       "5282         0.181818       0.153846                    0.0      0.532688   \n",
       "3872         0.133333       0.200000                    0.0      0.628854   \n",
       "...               ...            ...                    ...           ...   \n",
       "2854         0.120000       0.200000                    1.0      0.616516   \n",
       "4958         0.040000       0.200000                    0.0      0.552847   \n",
       "123          0.400000       0.200000                    0.0      0.000772   \n",
       "4000         0.022222       0.025641                    0.0      0.768054   \n",
       "4029         0.142857       0.125000                    0.0      0.410655   \n",
       "\n",
       "      Auction_Bids  Starting_Price_Average  Early_Bidding  Winning_Ratio  \n",
       "3830      0.000000                0.000000       0.002994       0.882353  \n",
       "2653      0.581395                0.993593       0.000009       0.000000  \n",
       "5693      0.000000                0.000000       0.000010       0.833333  \n",
       "5282      0.000000                0.000000       0.532688       1.000000  \n",
       "3872      0.000000                0.000000       0.108468       1.000000  \n",
       "...            ...                     ...            ...            ...  \n",
       "2854      0.400000                0.993593       0.616481       1.000000  \n",
       "4958      0.000000                0.000000       0.552847       1.000000  \n",
       "123       0.000000                0.000000       0.000772       1.000000  \n",
       "4000      0.538462                0.999935       0.768054       0.000000  \n",
       "4029      0.000000                0.000000       0.410655       0.750000  \n",
       "\n",
       "[1265 rows x 8 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_11 (Dense)             (None, 64)                576       \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,257\n",
      "Trainable params: 4,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4044 samples, validate on 1012 samples\n",
      "Epoch 1/1000\n",
      "4044/4044 [==============================] - 1s 224us/step - loss: 0.5017 - acc: 0.8126 - val_loss: 0.3560 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35598, saving model to ./model.h5\n",
      "Epoch 2/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.3399 - acc: 0.8952 - val_loss: 0.3405 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35598 to 0.34047, saving model to ./model.h5\n",
      "Epoch 3/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.3235 - acc: 0.8952 - val_loss: 0.3294 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34047 to 0.32939, saving model to ./model.h5\n",
      "Epoch 4/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.3110 - acc: 0.8952 - val_loss: 0.3127 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.32939 to 0.31267, saving model to ./model.h5\n",
      "Epoch 5/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.2883 - acc: 0.8952 - val_loss: 0.2829 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.31267 to 0.28287, saving model to ./model.h5\n",
      "Epoch 6/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.2422 - acc: 0.8956 - val_loss: 0.2055 - val_acc: 0.8992\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.28287 to 0.20549, saving model to ./model.h5\n",
      "Epoch 7/1000\n",
      "4044/4044 [==============================] - 0s 83us/step - loss: 0.1530 - acc: 0.9362 - val_loss: 0.1194 - val_acc: 0.9496\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.20549 to 0.11943, saving model to ./model.h5\n",
      "Epoch 8/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.0896 - acc: 0.9681 - val_loss: 0.0850 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.11943 to 0.08502, saving model to ./model.h5\n",
      "Epoch 9/1000\n",
      "4044/4044 [==============================] - 0s 65us/step - loss: 0.0660 - acc: 0.9728 - val_loss: 0.0707 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08502 to 0.07066, saving model to ./model.h5\n",
      "Epoch 10/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0564 - acc: 0.9740 - val_loss: 0.0655 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.07066 to 0.06552, saving model to ./model.h5\n",
      "Epoch 11/1000\n",
      "4044/4044 [==============================] - 0s 65us/step - loss: 0.0519 - acc: 0.9758 - val_loss: 0.0639 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06552 to 0.06392, saving model to ./model.h5\n",
      "Epoch 12/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0503 - acc: 0.9738 - val_loss: 0.0629 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.06392 to 0.06286, saving model to ./model.h5\n",
      "Epoch 13/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0494 - acc: 0.9760 - val_loss: 0.0637 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0489 - acc: 0.9748 - val_loss: 0.0624 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.06286 to 0.06243, saving model to ./model.h5\n",
      "Epoch 15/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0502 - acc: 0.9740 - val_loss: 0.0626 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/1000\n",
      "4044/4044 [==============================] - 0s 48us/step - loss: 0.0529 - acc: 0.9773 - val_loss: 0.0637 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0506 - acc: 0.9763 - val_loss: 0.0644 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0517 - acc: 0.9760 - val_loss: 0.0638 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0494 - acc: 0.9745 - val_loss: 0.0695 - val_acc: 0.9634\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0510 - acc: 0.9775 - val_loss: 0.0618 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.06243 to 0.06184, saving model to ./model.h5\n",
      "Epoch 21/1000\n",
      "4044/4044 [==============================] - 0s 68us/step - loss: 0.0513 - acc: 0.9735 - val_loss: 0.0633 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0473 - acc: 0.9780 - val_loss: 0.0654 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0492 - acc: 0.9760 - val_loss: 0.0639 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0466 - acc: 0.9780 - val_loss: 0.0600 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.06184 to 0.06001, saving model to ./model.h5\n",
      "Epoch 25/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0476 - acc: 0.9753 - val_loss: 0.0594 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.06001 to 0.05940, saving model to ./model.h5\n",
      "Epoch 26/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0465 - acc: 0.9763 - val_loss: 0.0601 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0457 - acc: 0.9775 - val_loss: 0.0611 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/1000\n",
      "4044/4044 [==============================] - 0s 48us/step - loss: 0.0461 - acc: 0.9768 - val_loss: 0.0594 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/1000\n",
      "4044/4044 [==============================] - 0s 49us/step - loss: 0.0456 - acc: 0.9760 - val_loss: 0.0613 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/1000\n",
      "4044/4044 [==============================] - 0s 48us/step - loss: 0.0454 - acc: 0.9753 - val_loss: 0.0592 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05940 to 0.05922, saving model to ./model.h5\n",
      "Epoch 31/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0454 - acc: 0.9765 - val_loss: 0.0591 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05922 to 0.05907, saving model to ./model.h5\n",
      "Epoch 32/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0442 - acc: 0.9785 - val_loss: 0.0591 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0453 - acc: 0.9753 - val_loss: 0.0593 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/1000\n",
      "4044/4044 [==============================] - 0s 47us/step - loss: 0.0469 - acc: 0.9773 - val_loss: 0.0590 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.05907 to 0.05903, saving model to ./model.h5\n",
      "Epoch 35/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0433 - acc: 0.9770 - val_loss: 0.0592 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4044/4044 [==============================] - 0s 49us/step - loss: 0.0460 - acc: 0.9750 - val_loss: 0.0595 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0456 - acc: 0.9765 - val_loss: 0.0614 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0448 - acc: 0.9770 - val_loss: 0.0605 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0460 - acc: 0.9763 - val_loss: 0.0591 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0449 - acc: 0.9768 - val_loss: 0.0585 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.05903 to 0.05853, saving model to ./model.h5\n",
      "Epoch 41/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0438 - acc: 0.9775 - val_loss: 0.0599 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0440 - acc: 0.9780 - val_loss: 0.0583 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.05853 to 0.05834, saving model to ./model.h5\n",
      "Epoch 43/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0451 - acc: 0.9760 - val_loss: 0.0614 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0447 - acc: 0.9797 - val_loss: 0.0595 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0450 - acc: 0.9768 - val_loss: 0.0596 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0459 - acc: 0.9768 - val_loss: 0.0588 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.0449 - acc: 0.9785 - val_loss: 0.0585 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0447 - acc: 0.9760 - val_loss: 0.0577 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.05834 to 0.05772, saving model to ./model.h5\n",
      "Epoch 49/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.0445 - acc: 0.9760 - val_loss: 0.0576 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00049: val_loss improved from 0.05772 to 0.05759, saving model to ./model.h5\n",
      "Epoch 50/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0460 - acc: 0.9753 - val_loss: 0.0611 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0439 - acc: 0.9768 - val_loss: 0.0576 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00051: val_loss improved from 0.05759 to 0.05757, saving model to ./model.h5\n",
      "Epoch 52/1000\n",
      "4044/4044 [==============================] - 0s 60us/step - loss: 0.0432 - acc: 0.9770 - val_loss: 0.0583 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0450 - acc: 0.9763 - val_loss: 0.0612 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/1000\n",
      "4044/4044 [==============================] - 0s 47us/step - loss: 0.0444 - acc: 0.9768 - val_loss: 0.0579 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/1000\n",
      "4044/4044 [==============================] - 0s 49us/step - loss: 0.0443 - acc: 0.9753 - val_loss: 0.0627 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/1000\n",
      "4044/4044 [==============================] - 0s 49us/step - loss: 0.0442 - acc: 0.9773 - val_loss: 0.0612 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0435 - acc: 0.9790 - val_loss: 0.0606 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0433 - acc: 0.9773 - val_loss: 0.0578 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0426 - acc: 0.9790 - val_loss: 0.0597 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0437 - acc: 0.9787 - val_loss: 0.0577 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0426 - acc: 0.9775 - val_loss: 0.0571 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00061: val_loss improved from 0.05757 to 0.05709, saving model to ./model.h5\n",
      "Epoch 62/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0434 - acc: 0.9777 - val_loss: 0.0570 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00062: val_loss improved from 0.05709 to 0.05701, saving model to ./model.h5\n",
      "Epoch 63/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.0434 - acc: 0.9775 - val_loss: 0.0578 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0449 - acc: 0.9755 - val_loss: 0.0579 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0436 - acc: 0.9768 - val_loss: 0.0564 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.05701 to 0.05639, saving model to ./model.h5\n",
      "Epoch 66/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0428 - acc: 0.9777 - val_loss: 0.0589 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00066: val_loss did not improve\n",
      "Epoch 67/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0431 - acc: 0.9780 - val_loss: 0.0568 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0441 - acc: 0.9782 - val_loss: 0.0573 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0426 - acc: 0.9770 - val_loss: 0.0562 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00069: val_loss improved from 0.05639 to 0.05623, saving model to ./model.h5\n",
      "Epoch 70/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0428 - acc: 0.9785 - val_loss: 0.0565 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0422 - acc: 0.9773 - val_loss: 0.0563 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0417 - acc: 0.9777 - val_loss: 0.0565 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0432 - acc: 0.9782 - val_loss: 0.0564 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0424 - acc: 0.9785 - val_loss: 0.0585 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0436 - acc: 0.9768 - val_loss: 0.0563 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0418 - acc: 0.9795 - val_loss: 0.0557 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00076: val_loss improved from 0.05623 to 0.05569, saving model to ./model.h5\n",
      "Epoch 77/1000\n",
      "4044/4044 [==============================] - 0s 60us/step - loss: 0.0445 - acc: 0.9763 - val_loss: 0.0557 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0443 - acc: 0.9768 - val_loss: 0.0559 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00078: val_loss did not improve\n",
      "Epoch 79/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0418 - acc: 0.9787 - val_loss: 0.0556 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.05569 to 0.05563, saving model to ./model.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/1000\n",
      "4044/4044 [==============================] - 0s 71us/step - loss: 0.0415 - acc: 0.9770 - val_loss: 0.0567 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0425 - acc: 0.9787 - val_loss: 0.0568 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0423 - acc: 0.9782 - val_loss: 0.0571 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0408 - acc: 0.9785 - val_loss: 0.0559 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0418 - acc: 0.9790 - val_loss: 0.0551 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00084: val_loss improved from 0.05563 to 0.05514, saving model to ./model.h5\n",
      "Epoch 85/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0402 - acc: 0.9792 - val_loss: 0.0578 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0422 - acc: 0.9792 - val_loss: 0.0553 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0414 - acc: 0.9785 - val_loss: 0.0550 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.05514 to 0.05498, saving model to ./model.h5\n",
      "Epoch 88/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0413 - acc: 0.9802 - val_loss: 0.0547 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00088: val_loss improved from 0.05498 to 0.05468, saving model to ./model.h5\n",
      "Epoch 89/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0408 - acc: 0.9787 - val_loss: 0.0579 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0424 - acc: 0.9790 - val_loss: 0.0547 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0414 - acc: 0.9780 - val_loss: 0.0558 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00091: val_loss did not improve\n",
      "Epoch 92/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0422 - acc: 0.9790 - val_loss: 0.0547 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0409 - acc: 0.9797 - val_loss: 0.0547 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/1000\n",
      "4044/4044 [==============================] - 0s 49us/step - loss: 0.0404 - acc: 0.9795 - val_loss: 0.0562 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/1000\n",
      "4044/4044 [==============================] - 0s 48us/step - loss: 0.0417 - acc: 0.9795 - val_loss: 0.0547 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0411 - acc: 0.9800 - val_loss: 0.0547 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00096: val_loss improved from 0.05468 to 0.05467, saving model to ./model.h5\n",
      "Epoch 97/1000\n",
      "4044/4044 [==============================] - 0s 60us/step - loss: 0.0420 - acc: 0.9782 - val_loss: 0.0541 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.05467 to 0.05405, saving model to ./model.h5\n",
      "Epoch 98/1000\n",
      "4044/4044 [==============================] - 0s 71us/step - loss: 0.0404 - acc: 0.9795 - val_loss: 0.0541 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0436 - acc: 0.9787 - val_loss: 0.0541 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.0411 - acc: 0.9785 - val_loss: 0.0542 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 101/1000\n",
      "4044/4044 [==============================] - 0s 71us/step - loss: 0.0408 - acc: 0.9792 - val_loss: 0.0541 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 102/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0400 - acc: 0.9797 - val_loss: 0.0540 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00102: val_loss improved from 0.05405 to 0.05402, saving model to ./model.h5\n",
      "Epoch 103/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0406 - acc: 0.9795 - val_loss: 0.0537 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00103: val_loss improved from 0.05402 to 0.05367, saving model to ./model.h5\n",
      "Epoch 104/1000\n",
      "4044/4044 [==============================] - 0s 65us/step - loss: 0.0410 - acc: 0.9800 - val_loss: 0.0539 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00104: val_loss did not improve\n",
      "Epoch 105/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0401 - acc: 0.9802 - val_loss: 0.0543 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00105: val_loss did not improve\n",
      "Epoch 106/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0414 - acc: 0.9795 - val_loss: 0.0540 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00106: val_loss did not improve\n",
      "Epoch 107/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0407 - acc: 0.9800 - val_loss: 0.0536 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00107: val_loss improved from 0.05367 to 0.05356, saving model to ./model.h5\n",
      "Epoch 108/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0401 - acc: 0.9790 - val_loss: 0.0548 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00108: val_loss did not improve\n",
      "Epoch 109/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0406 - acc: 0.9797 - val_loss: 0.0540 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00109: val_loss did not improve\n",
      "Epoch 110/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0403 - acc: 0.9800 - val_loss: 0.0541 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00110: val_loss did not improve\n",
      "Epoch 111/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.0397 - acc: 0.9802 - val_loss: 0.0542 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00111: val_loss did not improve\n",
      "Epoch 112/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0398 - acc: 0.9805 - val_loss: 0.0538 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00112: val_loss did not improve\n",
      "Epoch 113/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0407 - acc: 0.9807 - val_loss: 0.0537 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00113: val_loss did not improve\n",
      "Epoch 114/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0417 - acc: 0.9812 - val_loss: 0.0536 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00114: val_loss improved from 0.05356 to 0.05356, saving model to ./model.h5\n",
      "Epoch 115/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0404 - acc: 0.9805 - val_loss: 0.0536 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00115: val_loss improved from 0.05356 to 0.05355, saving model to ./model.h5\n",
      "Epoch 116/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0401 - acc: 0.9800 - val_loss: 0.0538 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00116: val_loss did not improve\n",
      "Epoch 117/1000\n",
      "4044/4044 [==============================] - 0s 60us/step - loss: 0.0402 - acc: 0.9805 - val_loss: 0.0541 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00117: val_loss did not improve\n",
      "Epoch 118/1000\n",
      "4044/4044 [==============================] - 0s 60us/step - loss: 0.0401 - acc: 0.9795 - val_loss: 0.0552 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00118: val_loss did not improve\n",
      "Epoch 119/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0409 - acc: 0.9800 - val_loss: 0.0536 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00119: val_loss improved from 0.05355 to 0.05355, saving model to ./model.h5\n",
      "Epoch 120/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0404 - acc: 0.9800 - val_loss: 0.0539 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00120: val_loss did not improve\n",
      "Epoch 121/1000\n",
      "4044/4044 [==============================] - 0s 60us/step - loss: 0.0397 - acc: 0.9797 - val_loss: 0.0531 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00121: val_loss improved from 0.05355 to 0.05314, saving model to ./model.h5\n",
      "Epoch 122/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0398 - acc: 0.9800 - val_loss: 0.0534 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00122: val_loss did not improve\n",
      "Epoch 123/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0411 - acc: 0.9802 - val_loss: 0.0529 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00123: val_loss improved from 0.05314 to 0.05295, saving model to ./model.h5\n",
      "Epoch 124/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0396 - acc: 0.9805 - val_loss: 0.0531 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00124: val_loss did not improve\n",
      "Epoch 125/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0399 - acc: 0.9805 - val_loss: 0.0527 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00125: val_loss improved from 0.05295 to 0.05268, saving model to ./model.h5\n",
      "Epoch 126/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0400 - acc: 0.9807 - val_loss: 0.0529 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00126: val_loss did not improve\n",
      "Epoch 127/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0404 - acc: 0.9802 - val_loss: 0.0552 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00127: val_loss did not improve\n",
      "Epoch 128/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0404 - acc: 0.9797 - val_loss: 0.0540 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00128: val_loss did not improve\n",
      "Epoch 129/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0390 - acc: 0.9810 - val_loss: 0.0533 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00129: val_loss did not improve\n",
      "Epoch 130/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0402 - acc: 0.9802 - val_loss: 0.0531 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00130: val_loss did not improve\n",
      "Epoch 131/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0393 - acc: 0.9802 - val_loss: 0.0533 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00131: val_loss did not improve\n",
      "Epoch 132/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0396 - acc: 0.9802 - val_loss: 0.0534 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00132: val_loss did not improve\n",
      "Epoch 133/1000\n",
      "4044/4044 [==============================] - 0s 49us/step - loss: 0.0393 - acc: 0.9810 - val_loss: 0.0542 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00133: val_loss did not improve\n",
      "Epoch 134/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0405 - acc: 0.9807 - val_loss: 0.0533 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00134: val_loss did not improve\n",
      "Epoch 135/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0397 - acc: 0.9802 - val_loss: 0.0533 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00135: val_loss did not improve\n",
      "Epoch 00135: early stopping\n",
      "[INFO] Best loss: 0.03895890286595779\n",
      "[INFO] Best acc: 0.9812067254832425\n",
      "[INFO] Best val_loss: 0.052680832212385925\n",
      "[INFO] Best val_acc: 0.976284587336152\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 建構 model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, input_shape=(X_train.shape[1],), activation=\"sigmoid\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.05))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 顯示模型摘要與結構\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, mode='auto', verbose=1)\n",
    "checkpointer = ModelCheckpoint('./model.h5',verbose=1, save_best_only=True)\n",
    "\n",
    "# 開始訓練 model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, batch_size=128, callbacks=[es,checkpointer])\n",
    "\n",
    "print(\"[INFO] Best loss: {}\".format(np.min(history.history['loss'])))\n",
    "print(\"[INFO] Best acc: {}\".format(np.max(history.history['acc'])))\n",
    "print(\"[INFO] Best val_loss: {}\".format(np.min(history.history['val_loss'])))\n",
    "print(\"[INFO] Best val_acc: {}\".format(np.max(history.history['val_acc'])))\n",
    "\n",
    "plt.plot(history.history['acc'], label='acc')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('Accuracy/Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 0s 74us/step\n",
      "Test Acc : 0.9826086956521739\n",
      "Test Loss : 0.0417844298909503\n"
     ]
    }
   ],
   "source": [
    "# 評估指標\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Acc : \" + str(accuracy))\n",
    "print(\"Test Loss : \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "        max_value=max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value==y_pred[i][j]:\n",
    "                y_pred[i][j]=1\n",
    "            else:\n",
    "                y_pred[i][j]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.10909090909090909\n",
      "Recall : 1.0\n",
      "F1 : 0.19672131147540983\n"
     ]
    }
   ],
   "source": [
    "print('Precision : ' + str(precision_score(y_true, y_pred)))\n",
    "print('Recall : ' + str(recall_score(y_true, y_pred)))\n",
    "print('F1 : ' + str(f1_score(y_true,  y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同節點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 128)               1152      \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 12,033\n",
      "Trainable params: 12,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4044 samples, validate on 1012 samples\n",
      "Epoch 1/1000\n",
      "4044/4044 [==============================] - 1s 232us/step - loss: 0.3892 - acc: 0.8942 - val_loss: 0.3483 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.34834, saving model to ./model.h5\n",
      "Epoch 2/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.3262 - acc: 0.8952 - val_loss: 0.3315 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.34834 to 0.33153, saving model to ./model.h5\n",
      "Epoch 3/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.3132 - acc: 0.8952 - val_loss: 0.3145 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.33153 to 0.31447, saving model to ./model.h5\n",
      "Epoch 4/1000\n",
      "4044/4044 [==============================] - 0s 72us/step - loss: 0.2876 - acc: 0.8952 - val_loss: 0.2715 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31447 to 0.27151, saving model to ./model.h5\n",
      "Epoch 5/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.2150 - acc: 0.9028 - val_loss: 0.1581 - val_acc: 0.9249\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.27151 to 0.15815, saving model to ./model.h5\n",
      "Epoch 6/1000\n",
      "4044/4044 [==============================] - 0s 60us/step - loss: 0.1014 - acc: 0.9676 - val_loss: 0.0790 - val_acc: 0.9674\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.15815 to 0.07904, saving model to ./model.h5\n",
      "Epoch 7/1000\n",
      "4044/4044 [==============================] - 0s 60us/step - loss: 0.0593 - acc: 0.9765 - val_loss: 0.0679 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07904 to 0.06787, saving model to ./model.h5\n",
      "Epoch 8/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0559 - acc: 0.9753 - val_loss: 0.0701 - val_acc: 0.9684\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/1000\n",
      "4044/4044 [==============================] - 0s 49us/step - loss: 0.0529 - acc: 0.9755 - val_loss: 0.0675 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06787 to 0.06747, saving model to ./model.h5\n",
      "Epoch 10/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0507 - acc: 0.9753 - val_loss: 0.0634 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06747 to 0.06344, saving model to ./model.h5\n",
      "Epoch 11/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0507 - acc: 0.9760 - val_loss: 0.0631 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.06344 to 0.06305, saving model to ./model.h5\n",
      "Epoch 12/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0504 - acc: 0.9728 - val_loss: 0.0702 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0513 - acc: 0.9758 - val_loss: 0.0626 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.06305 to 0.06259, saving model to ./model.h5\n",
      "Epoch 14/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0497 - acc: 0.9768 - val_loss: 0.0627 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0487 - acc: 0.9753 - val_loss: 0.0621 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.06259 to 0.06210, saving model to ./model.h5\n",
      "Epoch 16/1000\n",
      "4044/4044 [==============================] - 0s 69us/step - loss: 0.0493 - acc: 0.9750 - val_loss: 0.0646 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0489 - acc: 0.9768 - val_loss: 0.0632 - val_acc: 0.9704\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0504 - acc: 0.9753 - val_loss: 0.0615 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06210 to 0.06153, saving model to ./model.h5\n",
      "Epoch 19/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0495 - acc: 0.9755 - val_loss: 0.0603 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.06153 to 0.06025, saving model to ./model.h5\n",
      "Epoch 20/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0459 - acc: 0.9785 - val_loss: 0.0600 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.06025 to 0.05995, saving model to ./model.h5\n",
      "Epoch 21/1000\n",
      "4044/4044 [==============================] - 0s 65us/step - loss: 0.0480 - acc: 0.9750 - val_loss: 0.0600 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0517 - acc: 0.9768 - val_loss: 0.0648 - val_acc: 0.9664\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0487 - acc: 0.9755 - val_loss: 0.0599 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.05995 to 0.05988, saving model to ./model.h5\n",
      "Epoch 24/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0471 - acc: 0.9760 - val_loss: 0.0712 - val_acc: 0.9585\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n",
      "Epoch 25/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0488 - acc: 0.9755 - val_loss: 0.0591 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.05988 to 0.05907, saving model to ./model.h5\n",
      "Epoch 26/1000\n",
      "4044/4044 [==============================] - 0s 60us/step - loss: 0.0470 - acc: 0.9763 - val_loss: 0.0616 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.0478 - acc: 0.9760 - val_loss: 0.0600 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0454 - acc: 0.9763 - val_loss: 0.0591 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.05907 to 0.05907, saving model to ./model.h5\n",
      "Epoch 29/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0453 - acc: 0.9765 - val_loss: 0.0593 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0465 - acc: 0.9773 - val_loss: 0.0605 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0452 - acc: 0.9768 - val_loss: 0.0582 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.05907 to 0.05821, saving model to ./model.h5\n",
      "Epoch 32/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0442 - acc: 0.9755 - val_loss: 0.0582 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0467 - acc: 0.9763 - val_loss: 0.0643 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0506 - acc: 0.9738 - val_loss: 0.0576 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.05821 to 0.05761, saving model to ./model.h5\n",
      "Epoch 35/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0448 - acc: 0.9758 - val_loss: 0.0576 - val_acc: 0.9733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0444 - acc: 0.9755 - val_loss: 0.0618 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0475 - acc: 0.9750 - val_loss: 0.0644 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0479 - acc: 0.9765 - val_loss: 0.0600 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0451 - acc: 0.9807 - val_loss: 0.0569 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.05761 to 0.05692, saving model to ./model.h5\n",
      "Epoch 40/1000\n",
      "4044/4044 [==============================] - 0s 91us/step - loss: 0.0429 - acc: 0.9768 - val_loss: 0.0564 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00040: val_loss improved from 0.05692 to 0.05645, saving model to ./model.h5\n",
      "Epoch 41/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0443 - acc: 0.9790 - val_loss: 0.0574 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0440 - acc: 0.9760 - val_loss: 0.0569 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0435 - acc: 0.9773 - val_loss: 0.0564 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.05645 to 0.05642, saving model to ./model.h5\n",
      "Epoch 44/1000\n",
      "4044/4044 [==============================] - 0s 65us/step - loss: 0.0422 - acc: 0.9777 - val_loss: 0.0571 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0436 - acc: 0.9773 - val_loss: 0.0567 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0426 - acc: 0.9770 - val_loss: 0.0591 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0420 - acc: 0.9780 - val_loss: 0.0585 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0428 - acc: 0.9795 - val_loss: 0.0630 - val_acc: 0.9654\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0450 - acc: 0.9792 - val_loss: 0.0567 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0428 - acc: 0.9775 - val_loss: 0.0558 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.05642 to 0.05581, saving model to ./model.h5\n",
      "Epoch 51/1000\n",
      "4044/4044 [==============================] - 0s 74us/step - loss: 0.0439 - acc: 0.9763 - val_loss: 0.0562 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0439 - acc: 0.9775 - val_loss: 0.0554 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.05581 to 0.05539, saving model to ./model.h5\n",
      "Epoch 53/1000\n",
      "4044/4044 [==============================] - 0s 68us/step - loss: 0.0419 - acc: 0.9775 - val_loss: 0.0562 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 54/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.0428 - acc: 0.9800 - val_loss: 0.0557 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0437 - acc: 0.9770 - val_loss: 0.0583 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0438 - acc: 0.9768 - val_loss: 0.0586 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0410 - acc: 0.9770 - val_loss: 0.0554 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00057: val_loss improved from 0.05539 to 0.05539, saving model to ./model.h5\n",
      "Epoch 58/1000\n",
      "4044/4044 [==============================] - 0s 74us/step - loss: 0.0414 - acc: 0.9782 - val_loss: 0.0580 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00058: val_loss did not improve\n",
      "Epoch 59/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0429 - acc: 0.9790 - val_loss: 0.0562 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0420 - acc: 0.9782 - val_loss: 0.0554 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.05539 to 0.05539, saving model to ./model.h5\n",
      "Epoch 61/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0421 - acc: 0.9797 - val_loss: 0.0559 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0421 - acc: 0.9782 - val_loss: 0.0557 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0422 - acc: 0.9770 - val_loss: 0.0567 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0421 - acc: 0.9790 - val_loss: 0.0552 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00064: val_loss improved from 0.05539 to 0.05519, saving model to ./model.h5\n",
      "Epoch 65/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.0413 - acc: 0.9790 - val_loss: 0.0556 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0420 - acc: 0.9773 - val_loss: 0.0549 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.05519 to 0.05492, saving model to ./model.h5\n",
      "Epoch 67/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0417 - acc: 0.9797 - val_loss: 0.0547 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.05492 to 0.05466, saving model to ./model.h5\n",
      "Epoch 68/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0419 - acc: 0.9792 - val_loss: 0.0551 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0414 - acc: 0.9790 - val_loss: 0.0562 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0412 - acc: 0.9780 - val_loss: 0.0543 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.05466 to 0.05431, saving model to ./model.h5\n",
      "Epoch 71/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0416 - acc: 0.9782 - val_loss: 0.0552 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0414 - acc: 0.9782 - val_loss: 0.0547 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0401 - acc: 0.9802 - val_loss: 0.0578 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0418 - acc: 0.9777 - val_loss: 0.0553 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0412 - acc: 0.9792 - val_loss: 0.0547 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0405 - acc: 0.9790 - val_loss: 0.0544 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 77/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0408 - acc: 0.9787 - val_loss: 0.0551 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00077: val_loss did not improve\n",
      "Epoch 78/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0425 - acc: 0.9782 - val_loss: 0.0542 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00078: val_loss improved from 0.05431 to 0.05423, saving model to ./model.h5\n",
      "Epoch 79/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0409 - acc: 0.9802 - val_loss: 0.0539 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00079: val_loss improved from 0.05423 to 0.05389, saving model to ./model.h5\n",
      "Epoch 80/1000\n",
      "4044/4044 [==============================] - 0s 71us/step - loss: 0.0409 - acc: 0.9792 - val_loss: 0.0551 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00080: val_loss did not improve\n",
      "Epoch 81/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0406 - acc: 0.9800 - val_loss: 0.0548 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00081: val_loss did not improve\n",
      "Epoch 82/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0434 - acc: 0.9790 - val_loss: 0.0545 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00082: val_loss did not improve\n",
      "Epoch 83/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0410 - acc: 0.9777 - val_loss: 0.0549 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00083: val_loss did not improve\n",
      "Epoch 84/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0413 - acc: 0.9797 - val_loss: 0.0541 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00084: val_loss did not improve\n",
      "Epoch 85/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0409 - acc: 0.9790 - val_loss: 0.0546 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00085: val_loss did not improve\n",
      "Epoch 86/1000\n",
      "4044/4044 [==============================] - 0s 49us/step - loss: 0.0440 - acc: 0.9773 - val_loss: 0.0557 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00086: val_loss did not improve\n",
      "Epoch 87/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0410 - acc: 0.9787 - val_loss: 0.0538 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00087: val_loss improved from 0.05389 to 0.05378, saving model to ./model.h5\n",
      "Epoch 88/1000\n",
      "4044/4044 [==============================] - 0s 59us/step - loss: 0.0404 - acc: 0.9792 - val_loss: 0.0556 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00088: val_loss did not improve\n",
      "Epoch 89/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0420 - acc: 0.9792 - val_loss: 0.0539 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00089: val_loss did not improve\n",
      "Epoch 90/1000\n",
      "4044/4044 [==============================] - 0s 49us/step - loss: 0.0402 - acc: 0.9797 - val_loss: 0.0542 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00090: val_loss did not improve\n",
      "Epoch 91/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0404 - acc: 0.9802 - val_loss: 0.0538 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00091: val_loss improved from 0.05378 to 0.05377, saving model to ./model.h5\n",
      "Epoch 92/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0408 - acc: 0.9800 - val_loss: 0.0538 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00092: val_loss did not improve\n",
      "Epoch 93/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0407 - acc: 0.9780 - val_loss: 0.0541 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00093: val_loss did not improve\n",
      "Epoch 94/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0398 - acc: 0.9807 - val_loss: 0.0543 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00094: val_loss did not improve\n",
      "Epoch 95/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0403 - acc: 0.9787 - val_loss: 0.0555 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00095: val_loss did not improve\n",
      "Epoch 96/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0408 - acc: 0.9797 - val_loss: 0.0546 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00096: val_loss did not improve\n",
      "Epoch 97/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0396 - acc: 0.9802 - val_loss: 0.0559 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00097: val_loss did not improve\n",
      "Epoch 98/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0411 - acc: 0.9792 - val_loss: 0.0540 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00098: val_loss did not improve\n",
      "Epoch 99/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0395 - acc: 0.9800 - val_loss: 0.0541 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00099: val_loss did not improve\n",
      "Epoch 100/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0399 - acc: 0.9795 - val_loss: 0.0573 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00100: val_loss did not improve\n",
      "Epoch 101/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0402 - acc: 0.9807 - val_loss: 0.0561 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00101: val_loss did not improve\n",
      "Epoch 00101: early stopping\n",
      "[INFO] Best loss: 0.03953476413590677\n",
      "[INFO] Best acc: 0.9807121659952385\n",
      "[INFO] Best val_loss: 0.05377300905262529\n",
      "[INFO] Best val_acc: 0.976284587336152\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 建構 model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(128, input_shape=(X_train.shape[1],), activation=\"sigmoid\"))\n",
    "model.add(layers.Dense(64, activation=\"relu\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.05))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 顯示模型摘要與結構\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, mode='auto', verbose=1)\n",
    "checkpointer = ModelCheckpoint('./model.h5',verbose=1, save_best_only=True)\n",
    "\n",
    "# 開始訓練 model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, batch_size=128, callbacks=[es,checkpointer])\n",
    "\n",
    "print(\"[INFO] Best loss: {}\".format(np.min(history.history['loss'])))\n",
    "print(\"[INFO] Best acc: {}\".format(np.max(history.history['acc'])))\n",
    "print(\"[INFO] Best val_loss: {}\".format(np.min(history.history['val_loss'])))\n",
    "print(\"[INFO] Best val_acc: {}\".format(np.max(history.history['val_acc'])))\n",
    "\n",
    "plt.plot(history.history['acc'], label='acc')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('Accuracy/Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 0s 73us/step\n",
      "Test Acc : 0.9818181818181818\n",
      "Test Loss : 0.044491258341535604\n"
     ]
    }
   ],
   "source": [
    "# 評估指標\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Acc : \" + str(accuracy))\n",
    "print(\"Test Loss : \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "        max_value=max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value==y_pred[i][j]:\n",
    "                y_pred[i][j]=1\n",
    "            else:\n",
    "                y_pred[i][j]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.10909090909090909\n",
      "Recall : 1.0\n",
      "F1 : 0.19672131147540983\n"
     ]
    }
   ],
   "source": [
    "print('Precision : ' + str(precision_score(y_true, y_pred)))\n",
    "print('Recall : ' + str(recall_score(y_true, y_pred)))\n",
    "print('F1 : ' + str(f1_score(y_true,  y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 不同層數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 64)                576       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 6,369\n",
      "Trainable params: 6,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4044 samples, validate on 1012 samples\n",
      "Epoch 1/1000\n",
      "4044/4044 [==============================] - 1s 305us/step - loss: 0.5319 - acc: 0.8670 - val_loss: 0.3537 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.35372, saving model to ./model.h5\n",
      "Epoch 2/1000\n",
      "4044/4044 [==============================] - 0s 69us/step - loss: 0.3457 - acc: 0.8952 - val_loss: 0.3430 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.35372 to 0.34296, saving model to ./model.h5\n",
      "Epoch 3/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.3227 - acc: 0.8952 - val_loss: 0.3186 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.34296 to 0.31860, saving model to ./model.h5\n",
      "Epoch 4/1000\n",
      "4044/4044 [==============================] - 0s 73us/step - loss: 0.2727 - acc: 0.8952 - val_loss: 0.2343 - val_acc: 0.8883\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.31860 to 0.23426, saving model to ./model.h5\n",
      "Epoch 5/1000\n",
      "4044/4044 [==============================] - 0s 73us/step - loss: 0.1531 - acc: 0.9201 - val_loss: 0.1100 - val_acc: 0.9545\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.23426 to 0.10999, saving model to ./model.h5\n",
      "Epoch 6/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0733 - acc: 0.9735 - val_loss: 0.0839 - val_acc: 0.9595\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10999 to 0.08388, saving model to ./model.h5\n",
      "Epoch 7/1000\n",
      "4044/4044 [==============================] - 0s 72us/step - loss: 0.0549 - acc: 0.9773 - val_loss: 0.0624 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08388 to 0.06238, saving model to ./model.h5\n",
      "Epoch 8/1000\n",
      "4044/4044 [==============================] - 0s 65us/step - loss: 0.0495 - acc: 0.9760 - val_loss: 0.0605 - val_acc: 0.9713\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06238 to 0.06047, saving model to ./model.h5\n",
      "Epoch 9/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0503 - acc: 0.9750 - val_loss: 0.0594 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06047 to 0.05942, saving model to ./model.h5\n",
      "Epoch 10/1000\n",
      "4044/4044 [==============================] - 0s 72us/step - loss: 0.0479 - acc: 0.9760 - val_loss: 0.0603 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0509 - acc: 0.9733 - val_loss: 0.0585 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.05942 to 0.05846, saving model to ./model.h5\n",
      "Epoch 12/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0454 - acc: 0.9792 - val_loss: 0.0578 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.05846 to 0.05779, saving model to ./model.h5\n",
      "Epoch 13/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0433 - acc: 0.9782 - val_loss: 0.0583 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0437 - acc: 0.9775 - val_loss: 0.0593 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0447 - acc: 0.9790 - val_loss: 0.0588 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0435 - acc: 0.9775 - val_loss: 0.0591 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0448 - acc: 0.9782 - val_loss: 0.0579 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0447 - acc: 0.9763 - val_loss: 0.0566 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.05779 to 0.05664, saving model to ./model.h5\n",
      "Epoch 19/1000\n",
      "4044/4044 [==============================] - 0s 69us/step - loss: 0.0435 - acc: 0.9790 - val_loss: 0.0589 - val_acc: 0.9723\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0418 - acc: 0.9797 - val_loss: 0.0576 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0447 - acc: 0.9763 - val_loss: 0.0561 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.05664 to 0.05612, saving model to ./model.h5\n",
      "Epoch 22/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0498 - acc: 0.9760 - val_loss: 0.0573 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0432 - acc: 0.9807 - val_loss: 0.0561 - val_acc: 0.9733\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "Epoch 24/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0417 - acc: 0.9787 - val_loss: 0.0556 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.05612 to 0.05557, saving model to ./model.h5\n",
      "Epoch 25/1000\n",
      "4044/4044 [==============================] - 0s 70us/step - loss: 0.0435 - acc: 0.9763 - val_loss: 0.0574 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/1000\n",
      "4044/4044 [==============================] - 0s 68us/step - loss: 0.0418 - acc: 0.9790 - val_loss: 0.0580 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00026: val_loss did not improve\n",
      "Epoch 27/1000\n",
      "4044/4044 [==============================] - 0s 69us/step - loss: 0.0427 - acc: 0.9782 - val_loss: 0.0557 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/1000\n",
      "4044/4044 [==============================] - 0s 75us/step - loss: 0.0432 - acc: 0.9777 - val_loss: 0.0559 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00028: val_loss did not improve\n",
      "Epoch 29/1000\n",
      "4044/4044 [==============================] - 0s 74us/step - loss: 0.0429 - acc: 0.9785 - val_loss: 0.0558 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/1000\n",
      "4044/4044 [==============================] - 0s 70us/step - loss: 0.0417 - acc: 0.9780 - val_loss: 0.0547 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.05557 to 0.05470, saving model to ./model.h5\n",
      "Epoch 31/1000\n",
      "4044/4044 [==============================] - 0s 80us/step - loss: 0.0423 - acc: 0.9805 - val_loss: 0.0548 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0412 - acc: 0.9787 - val_loss: 0.0552 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/1000\n",
      "4044/4044 [==============================] - 0s 72us/step - loss: 0.0409 - acc: 0.9780 - val_loss: 0.0550 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0423 - acc: 0.9785 - val_loss: 0.0545 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.05470 to 0.05445, saving model to ./model.h5\n",
      "Epoch 35/1000\n",
      "4044/4044 [==============================] - 0s 77us/step - loss: 0.0421 - acc: 0.9792 - val_loss: 0.0578 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/1000\n",
      "4044/4044 [==============================] - 0s 75us/step - loss: 0.0427 - acc: 0.9785 - val_loss: 0.0581 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/1000\n",
      "4044/4044 [==============================] - 0s 70us/step - loss: 0.0422 - acc: 0.9780 - val_loss: 0.0552 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/1000\n",
      "4044/4044 [==============================] - 0s 83us/step - loss: 0.0412 - acc: 0.9805 - val_loss: 0.0568 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/1000\n",
      "4044/4044 [==============================] - 0s 73us/step - loss: 0.0428 - acc: 0.9785 - val_loss: 0.0555 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/1000\n",
      "4044/4044 [==============================] - 0s 70us/step - loss: 0.0408 - acc: 0.9790 - val_loss: 0.0549 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0422 - acc: 0.9810 - val_loss: 0.0581 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00041: val_loss did not improve\n",
      "Epoch 42/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0409 - acc: 0.9812 - val_loss: 0.0550 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0415 - acc: 0.9795 - val_loss: 0.0540 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.05445 to 0.05401, saving model to ./model.h5\n",
      "Epoch 44/1000\n",
      "4044/4044 [==============================] - 0s 70us/step - loss: 0.0409 - acc: 0.9800 - val_loss: 0.0544 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0430 - acc: 0.9797 - val_loss: 0.0547 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/1000\n",
      "4044/4044 [==============================] - 0s 69us/step - loss: 0.0413 - acc: 0.9795 - val_loss: 0.0550 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00046: val_loss did not improve\n",
      "Epoch 47/1000\n",
      "4044/4044 [==============================] - 0s 64us/step - loss: 0.0402 - acc: 0.9795 - val_loss: 0.0550 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00047: val_loss did not improve\n",
      "Epoch 48/1000\n",
      "4044/4044 [==============================] - 0s 68us/step - loss: 0.0418 - acc: 0.9795 - val_loss: 0.0543 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0395 - acc: 0.9795 - val_loss: 0.0554 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/1000\n",
      "4044/4044 [==============================] - 0s 69us/step - loss: 0.0424 - acc: 0.9807 - val_loss: 0.0548 - val_acc: 0.9753\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0406 - acc: 0.9802 - val_loss: 0.0560 - val_acc: 0.9743\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0466 - acc: 0.9782 - val_loss: 0.0546 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/1000\n",
      "4044/4044 [==============================] - 0s 68us/step - loss: 0.0432 - acc: 0.9787 - val_loss: 0.0542 - val_acc: 0.9763\n",
      "\n",
      "Epoch 00053: val_loss did not improve\n",
      "Epoch 00053: early stopping\n",
      "[INFO] Best loss: 0.039491450150601355\n",
      "[INFO] Best acc: 0.9812067258369793\n",
      "[INFO] Best val_loss: 0.054007841296228966\n",
      "[INFO] Best val_acc: 0.976284587336152\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 建構 model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, input_shape=(X_train.shape[1],), activation=\"sigmoid\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.05))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# opt = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# 顯示模型摘要與結構\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, mode='auto', verbose=1)\n",
    "checkpointer = ModelCheckpoint('./model.h5',verbose=1, save_best_only=True)\n",
    "\n",
    "# 開始訓練 model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, batch_size=128, callbacks=[es,checkpointer])\n",
    "\n",
    "print(\"[INFO] Best loss: {}\".format(np.min(history.history['loss'])))\n",
    "print(\"[INFO] Best acc: {}\".format(np.max(history.history['acc'])))\n",
    "print(\"[INFO] Best val_loss: {}\".format(np.min(history.history['val_loss'])))\n",
    "print(\"[INFO] Best val_acc: {}\".format(np.max(history.history['val_acc'])))\n",
    "\n",
    "plt.plot(history.history['acc'], label='acc')\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('Accuracy/Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 0s 73us/step\n",
      "Test Acc : 0.9826086956521739\n",
      "Test Loss : 0.04205434046411453\n"
     ]
    }
   ],
   "source": [
    "# 評估指標\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Acc : \" + str(accuracy))\n",
    "print(\"Test Loss : \" + str(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_true = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(y_pred)):\n",
    "        max_value=max(y_pred[i])\n",
    "        for j in range(len(y_pred[i])):\n",
    "            if max_value==y_pred[i][j]:\n",
    "                y_pred[i][j]=1\n",
    "            else:\n",
    "                y_pred[i][j]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision : 0.10909090909090909\n",
      "Recall : 1.0\n",
      "F1 : 0.19672131147540983\n"
     ]
    }
   ],
   "source": [
    "print('Precision : ' + str(precision_score(y_true, y_pred)))\n",
    "print('Recall : ' + str(recall_score(y_true, y_pred)))\n",
    "print('F1 : ' + str(f1_score(y_true,  y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 預測數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Record_ID</th>\n",
       "      <th>Auction_ID</th>\n",
       "      <th>Bidder_ID</th>\n",
       "      <th>Bidder_Tendency</th>\n",
       "      <th>Bidding_Ratio</th>\n",
       "      <th>Successive_Outbidding</th>\n",
       "      <th>Last_Bidding</th>\n",
       "      <th>Auction_Bids</th>\n",
       "      <th>Starting_Price_Average</th>\n",
       "      <th>Early_Bidding</th>\n",
       "      <th>Winning_Ratio</th>\n",
       "      <th>Auction_Duration</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>732</td>\n",
       "      <td>_***i</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>732</td>\n",
       "      <td>g***r</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>732</td>\n",
       "      <td>t***p</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>732</td>\n",
       "      <td>7***n</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>900</td>\n",
       "      <td>z***z</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316</th>\n",
       "      <td>15129</td>\n",
       "      <td>760</td>\n",
       "      <td>l***t</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.738557</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.686358</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>15137</td>\n",
       "      <td>2481</td>\n",
       "      <td>s***s</td>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.878788</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6318</th>\n",
       "      <td>15138</td>\n",
       "      <td>2481</td>\n",
       "      <td>h***t</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015663</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.015663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6319</th>\n",
       "      <td>15139</td>\n",
       "      <td>2481</td>\n",
       "      <td>d***d</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068694</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6320</th>\n",
       "      <td>15144</td>\n",
       "      <td>2481</td>\n",
       "      <td>a***l</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.340351</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.340351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6321 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Record_ID  Auction_ID Bidder_ID  Bidder_Tendency  Bidding_Ratio  \\\n",
       "0             1         732     _***i         0.200000       0.400000   \n",
       "1             2         732     g***r         0.024390       0.200000   \n",
       "2             3         732     t***p         0.142857       0.200000   \n",
       "3             4         732     7***n         0.100000       0.200000   \n",
       "4             5         900     z***z         0.051282       0.222222   \n",
       "...         ...         ...       ...              ...            ...   \n",
       "6316      15129         760     l***t         0.333333       0.160000   \n",
       "6317      15137        2481     s***s         0.030612       0.130435   \n",
       "6318      15138        2481     h***t         0.055556       0.043478   \n",
       "6319      15139        2481     d***d         0.076923       0.086957   \n",
       "6320      15144        2481     a***l         0.016393       0.043478   \n",
       "\n",
       "      Successive_Outbidding  Last_Bidding  Auction_Bids  \\\n",
       "0                       0.0      0.000028      0.000000   \n",
       "1                       0.0      0.013123      0.000000   \n",
       "2                       0.0      0.003042      0.000000   \n",
       "3                       0.0      0.097477      0.000000   \n",
       "4                       0.0      0.001318      0.000000   \n",
       "...                     ...           ...           ...   \n",
       "6316                    1.0      0.738557      0.280000   \n",
       "6317                    0.0      0.005754      0.217391   \n",
       "6318                    0.0      0.015663      0.217391   \n",
       "6319                    0.0      0.068694      0.217391   \n",
       "6320                    0.0      0.340351      0.217391   \n",
       "\n",
       "      Starting_Price_Average  Early_Bidding  Winning_Ratio  Auction_Duration  \\\n",
       "0                   0.993593       0.000028       0.666667                 5   \n",
       "1                   0.993593       0.013123       0.944444                 5   \n",
       "2                   0.993593       0.003042       1.000000                 5   \n",
       "3                   0.993593       0.097477       1.000000                 5   \n",
       "4                   0.000000       0.001242       0.500000                 7   \n",
       "...                      ...            ...            ...               ...   \n",
       "6316                0.993593       0.686358       0.888889                 3   \n",
       "6317                0.993593       0.000010       0.878788                 7   \n",
       "6318                0.993593       0.015663       0.000000                 7   \n",
       "6319                0.993593       0.000415       0.000000                 7   \n",
       "6320                0.993593       0.340351       0.000000                 7   \n",
       "\n",
       "      Class  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "6316      1  \n",
       "6317      0  \n",
       "6318      0  \n",
       "6319      0  \n",
       "6320      0  \n",
       "\n",
       "[6321 rows x 13 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pandas.read_csv(\"Shill Bidding Dataset.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bidder_Tendency</th>\n",
       "      <th>Bidding_Ratio</th>\n",
       "      <th>Successive_Outbidding</th>\n",
       "      <th>Last_Bidding</th>\n",
       "      <th>Auction_Bids</th>\n",
       "      <th>Starting_Price_Average</th>\n",
       "      <th>Early_Bidding</th>\n",
       "      <th>Winning_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.013123</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.097477</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6316</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.738557</td>\n",
       "      <td>0.280000</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.686358</td>\n",
       "      <td>0.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>0.030612</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.878788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6318</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015663</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.015663</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6319</th>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.068694</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6320</th>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.340351</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.993593</td>\n",
       "      <td>0.340351</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6321 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Bidder_Tendency  Bidding_Ratio  Successive_Outbidding  Last_Bidding  \\\n",
       "0            0.200000       0.400000                    0.0      0.000028   \n",
       "1            0.024390       0.200000                    0.0      0.013123   \n",
       "2            0.142857       0.200000                    0.0      0.003042   \n",
       "3            0.100000       0.200000                    0.0      0.097477   \n",
       "4            0.051282       0.222222                    0.0      0.001318   \n",
       "...               ...            ...                    ...           ...   \n",
       "6316         0.333333       0.160000                    1.0      0.738557   \n",
       "6317         0.030612       0.130435                    0.0      0.005754   \n",
       "6318         0.055556       0.043478                    0.0      0.015663   \n",
       "6319         0.076923       0.086957                    0.0      0.068694   \n",
       "6320         0.016393       0.043478                    0.0      0.340351   \n",
       "\n",
       "      Auction_Bids  Starting_Price_Average  Early_Bidding  Winning_Ratio  \n",
       "0         0.000000                0.993593       0.000028       0.666667  \n",
       "1         0.000000                0.993593       0.013123       0.944444  \n",
       "2         0.000000                0.993593       0.003042       1.000000  \n",
       "3         0.000000                0.993593       0.097477       1.000000  \n",
       "4         0.000000                0.000000       0.001242       0.500000  \n",
       "...            ...                     ...            ...            ...  \n",
       "6316      0.280000                0.993593       0.686358       0.888889  \n",
       "6317      0.217391                0.993593       0.000010       0.878788  \n",
       "6318      0.217391                0.993593       0.015663       0.000000  \n",
       "6319      0.217391                0.993593       0.000415       0.000000  \n",
       "6320      0.217391                0.993593       0.340351       0.000000  \n",
       "\n",
       "[6321 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['Record_ID', 'Auction_ID', 'Bidder_ID', 'Auction_Duration','Class'], axis = 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['Winning_Ratio']\n",
    "X = data.drop(['Winning_Ratio'],axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_error(actual, predicted):\n",
    "    res = np.empty(actual.shape)\n",
    "    for j in range(actual.shape[0]):\n",
    "        if actual[j] != 0:\n",
    "            res[j] = (actual[j] - predicted[j]) / actual[j]\n",
    "        else:\n",
    "            res[j] = predicted[j] / np.mean(actual)\n",
    "    return res\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs(percentage_error(np.asarray(y_true), np.asarray(y_pred)))) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 64)                512       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 4,193\n",
      "Trainable params: 4,193\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 4044 samples, validate on 1012 samples\n",
      "Epoch 1/1000\n",
      "4044/4044 [==============================] - 1s 191us/step - loss: 0.2074 - mean_absolute_error: 0.3868 - mean_absolute_percentage_error: 134912551.0188 - val_loss: 0.1222 - val_mean_absolute_error: 0.3080 - val_mean_absolute_percentage_error: 142828866.6245\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.12223, saving model to ./model.h5\n",
      "Epoch 2/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.1058 - mean_absolute_error: 0.2605 - mean_absolute_percentage_error: 121819969.1236 - val_loss: 0.0934 - val_mean_absolute_error: 0.2380 - val_mean_absolute_percentage_error: 98792166.4190\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.12223 to 0.09337, saving model to ./model.h5\n",
      "Epoch 3/1000\n",
      "4044/4044 [==============================] - 0s 70us/step - loss: 0.0870 - mean_absolute_error: 0.2252 - mean_absolute_percentage_error: 98542262.6390 - val_loss: 0.0800 - val_mean_absolute_error: 0.2157 - val_mean_absolute_percentage_error: 94893485.0593\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.09337 to 0.07999, saving model to ./model.h5\n",
      "Epoch 4/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0755 - mean_absolute_error: 0.2059 - mean_absolute_percentage_error: 86979834.8249 - val_loss: 0.0680 - val_mean_absolute_error: 0.1910 - val_mean_absolute_percentage_error: 68717819.5889\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.07999 to 0.06804, saving model to ./model.h5\n",
      "Epoch 5/1000\n",
      "4044/4044 [==============================] - 0s 88us/step - loss: 0.0648 - mean_absolute_error: 0.1873 - mean_absolute_percentage_error: 78397529.7962 - val_loss: 0.0581 - val_mean_absolute_error: 0.1707 - val_mean_absolute_percentage_error: 56793948.0474\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.06804 to 0.05810, saving model to ./model.h5\n",
      "Epoch 6/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0546 - mean_absolute_error: 0.1649 - mean_absolute_percentage_error: 64915388.8783 - val_loss: 0.0448 - val_mean_absolute_error: 0.1517 - val_mean_absolute_percentage_error: 62683331.2885\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.05810 to 0.04485, saving model to ./model.h5\n",
      "Epoch 7/1000\n",
      "4044/4044 [==============================] - 0s 91us/step - loss: 0.0459 - mean_absolute_error: 0.1480 - mean_absolute_percentage_error: 59494581.3729 - val_loss: 0.0391 - val_mean_absolute_error: 0.1374 - val_mean_absolute_percentage_error: 54754084.8854\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.04485 to 0.03910, saving model to ./model.h5\n",
      "Epoch 8/1000\n",
      "4044/4044 [==============================] - 0s 71us/step - loss: 0.0399 - mean_absolute_error: 0.1338 - mean_absolute_percentage_error: 54344307.7112 - val_loss: 0.0329 - val_mean_absolute_error: 0.1186 - val_mean_absolute_percentage_error: 39583557.5336\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.03910 to 0.03289, saving model to ./model.h5\n",
      "Epoch 9/1000\n",
      "4044/4044 [==============================] - 0s 74us/step - loss: 0.0352 - mean_absolute_error: 0.1205 - mean_absolute_percentage_error: 45551486.8051 - val_loss: 0.0282 - val_mean_absolute_error: 0.1059 - val_mean_absolute_percentage_error: 39123184.3794\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.03289 to 0.02817, saving model to ./model.h5\n",
      "Epoch 10/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0329 - mean_absolute_error: 0.1125 - mean_absolute_percentage_error: 42069213.6142 - val_loss: 0.0297 - val_mean_absolute_error: 0.1058 - val_mean_absolute_percentage_error: 47662354.5296\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0288 - mean_absolute_error: 0.1026 - mean_absolute_percentage_error: 36434340.1523 - val_loss: 0.0243 - val_mean_absolute_error: 0.0925 - val_mean_absolute_percentage_error: 27153103.7075\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.02817 to 0.02431, saving model to ./model.h5\n",
      "Epoch 12/1000\n",
      "4044/4044 [==============================] - 0s 65us/step - loss: 0.0265 - mean_absolute_error: 0.0967 - mean_absolute_percentage_error: 31913317.5846 - val_loss: 0.0252 - val_mean_absolute_error: 0.0921 - val_mean_absolute_percentage_error: 39638561.5889\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0249 - mean_absolute_error: 0.0926 - mean_absolute_percentage_error: 31459013.6499 - val_loss: 0.0221 - val_mean_absolute_error: 0.0830 - val_mean_absolute_percentage_error: 21830075.3004\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.02431 to 0.02209, saving model to ./model.h5\n",
      "Epoch 14/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0235 - mean_absolute_error: 0.0870 - mean_absolute_percentage_error: 26570032.6884 - val_loss: 0.0209 - val_mean_absolute_error: 0.0782 - val_mean_absolute_percentage_error: 19818233.1700\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.02209 to 0.02089, saving model to ./model.h5\n",
      "Epoch 15/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0233 - mean_absolute_error: 0.0858 - mean_absolute_percentage_error: 26763827.9228 - val_loss: 0.0225 - val_mean_absolute_error: 0.0853 - val_mean_absolute_percentage_error: 34616875.7628\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/1000\n",
      "4044/4044 [==============================] - 0s 60us/step - loss: 0.0214 - mean_absolute_error: 0.0825 - mean_absolute_percentage_error: 25507155.0682 - val_loss: 0.0210 - val_mean_absolute_error: 0.0758 - val_mean_absolute_percentage_error: 16772159.4625\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0213 - mean_absolute_error: 0.0813 - mean_absolute_percentage_error: 24439452.1296 - val_loss: 0.0205 - val_mean_absolute_error: 0.0755 - val_mean_absolute_percentage_error: 14625860.8043\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.02089 to 0.02050, saving model to ./model.h5\n",
      "Epoch 18/1000\n",
      "4044/4044 [==============================] - 0s 65us/step - loss: 0.0196 - mean_absolute_error: 0.0782 - mean_absolute_percentage_error: 22570130.4253 - val_loss: 0.0180 - val_mean_absolute_error: 0.0725 - val_mean_absolute_percentage_error: 22384508.0000\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.02050 to 0.01798, saving model to ./model.h5\n",
      "Epoch 19/1000\n",
      "4044/4044 [==============================] - 0s 67us/step - loss: 0.0183 - mean_absolute_error: 0.0756 - mean_absolute_percentage_error: 20927523.7745 - val_loss: 0.0186 - val_mean_absolute_error: 0.0726 - val_mean_absolute_percentage_error: 17762415.8458\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0192 - mean_absolute_error: 0.0753 - mean_absolute_percentage_error: 21485361.9461 - val_loss: 0.0217 - val_mean_absolute_error: 0.0737 - val_mean_absolute_percentage_error: 13448161.8083\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "Epoch 21/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0171 - mean_absolute_error: 0.0721 - mean_absolute_percentage_error: 18893951.3729 - val_loss: 0.0206 - val_mean_absolute_error: 0.0737 - val_mean_absolute_percentage_error: 15990419.8972\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "Epoch 22/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0174 - mean_absolute_error: 0.0712 - mean_absolute_percentage_error: 18957601.1058 - val_loss: 0.0181 - val_mean_absolute_error: 0.0714 - val_mean_absolute_percentage_error: 22312775.6838\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "Epoch 23/1000\n",
      "4044/4044 [==============================] - 0s 47us/step - loss: 0.0169 - mean_absolute_error: 0.0705 - mean_absolute_percentage_error: 18261076.3185 - val_loss: 0.0177 - val_mean_absolute_error: 0.0691 - val_mean_absolute_percentage_error: 12575656.3083\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01798 to 0.01772, saving model to ./model.h5\n",
      "Epoch 24/1000\n",
      "4044/4044 [==============================] - 0s 45us/step - loss: 0.0162 - mean_absolute_error: 0.0687 - mean_absolute_percentage_error: 16532054.6271 - val_loss: 0.0174 - val_mean_absolute_error: 0.0700 - val_mean_absolute_percentage_error: 14790201.2866\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01772 to 0.01742, saving model to ./model.h5\n",
      "Epoch 25/1000\n",
      "4044/4044 [==============================] - 0s 47us/step - loss: 0.0155 - mean_absolute_error: 0.0674 - mean_absolute_percentage_error: 17022562.1919 - val_loss: 0.0218 - val_mean_absolute_error: 0.0738 - val_mean_absolute_percentage_error: 12444614.1561\n",
      "\n",
      "Epoch 00025: val_loss did not improve\n",
      "Epoch 26/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0177 - mean_absolute_error: 0.0697 - mean_absolute_percentage_error: 17134488.1128 - val_loss: 0.0169 - val_mean_absolute_error: 0.0664 - val_mean_absolute_percentage_error: 14716957.1897\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01742 to 0.01691, saving model to ./model.h5\n",
      "Epoch 27/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0156 - mean_absolute_error: 0.0661 - mean_absolute_percentage_error: 16540652.1345 - val_loss: 0.0183 - val_mean_absolute_error: 0.0692 - val_mean_absolute_percentage_error: 22875946.4664\n",
      "\n",
      "Epoch 00027: val_loss did not improve\n",
      "Epoch 28/1000\n",
      "4044/4044 [==============================] - 0s 47us/step - loss: 0.0162 - mean_absolute_error: 0.0657 - mean_absolute_percentage_error: 15486177.9130 - val_loss: 0.0159 - val_mean_absolute_error: 0.0656 - val_mean_absolute_percentage_error: 17189364.3202\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01691 to 0.01585, saving model to ./model.h5\n",
      "Epoch 29/1000\n",
      "4044/4044 [==============================] - 0s 63us/step - loss: 0.0144 - mean_absolute_error: 0.0639 - mean_absolute_percentage_error: 14864660.3853 - val_loss: 0.0173 - val_mean_absolute_error: 0.0674 - val_mean_absolute_percentage_error: 11712963.6700\n",
      "\n",
      "Epoch 00029: val_loss did not improve\n",
      "Epoch 30/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0134 - mean_absolute_error: 0.0611 - mean_absolute_percentage_error: 12962301.6696 - val_loss: 0.0160 - val_mean_absolute_error: 0.0630 - val_mean_absolute_percentage_error: 15093874.1304\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n",
      "Epoch 31/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0141 - mean_absolute_error: 0.0620 - mean_absolute_percentage_error: 13028639.7102 - val_loss: 0.0162 - val_mean_absolute_error: 0.0630 - val_mean_absolute_percentage_error: 14608960.6443\n",
      "\n",
      "Epoch 00031: val_loss did not improve\n",
      "Epoch 32/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0156 - mean_absolute_error: 0.0636 - mean_absolute_percentage_error: 15390931.4045 - val_loss: 0.0178 - val_mean_absolute_error: 0.0656 - val_mean_absolute_percentage_error: 9105040.3271\n",
      "\n",
      "Epoch 00032: val_loss did not improve\n",
      "Epoch 33/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0158 - mean_absolute_error: 0.0631 - mean_absolute_percentage_error: 13378364.5702 - val_loss: 0.0161 - val_mean_absolute_error: 0.0642 - val_mean_absolute_percentage_error: 15079327.4743\n",
      "\n",
      "Epoch 00033: val_loss did not improve\n",
      "Epoch 34/1000\n",
      "4044/4044 [==============================] - 0s 44us/step - loss: 0.0137 - mean_absolute_error: 0.0595 - mean_absolute_percentage_error: 12902270.0732 - val_loss: 0.0151 - val_mean_absolute_error: 0.0617 - val_mean_absolute_percentage_error: 12569103.7115\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01585 to 0.01507, saving model to ./model.h5\n",
      "Epoch 35/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0139 - mean_absolute_error: 0.0594 - mean_absolute_percentage_error: 11699289.9768 - val_loss: 0.0151 - val_mean_absolute_error: 0.0616 - val_mean_absolute_percentage_error: 10601155.1581\n",
      "\n",
      "Epoch 00035: val_loss did not improve\n",
      "Epoch 36/1000\n",
      "4044/4044 [==============================] - 0s 55us/step - loss: 0.0149 - mean_absolute_error: 0.0611 - mean_absolute_percentage_error: 12859628.9036 - val_loss: 0.0171 - val_mean_absolute_error: 0.0659 - val_mean_absolute_percentage_error: 18238597.6957\n",
      "\n",
      "Epoch 00036: val_loss did not improve\n",
      "Epoch 37/1000\n",
      "4044/4044 [==============================] - 0s 66us/step - loss: 0.0159 - mean_absolute_error: 0.0613 - mean_absolute_percentage_error: 12898165.3343 - val_loss: 0.0181 - val_mean_absolute_error: 0.0683 - val_mean_absolute_percentage_error: 20814031.1542\n",
      "\n",
      "Epoch 00037: val_loss did not improve\n",
      "Epoch 38/1000\n",
      "4044/4044 [==============================] - 0s 61us/step - loss: 0.0157 - mean_absolute_error: 0.0618 - mean_absolute_percentage_error: 13946913.6103 - val_loss: 0.0153 - val_mean_absolute_error: 0.0604 - val_mean_absolute_percentage_error: 13135658.0613\n",
      "\n",
      "Epoch 00038: val_loss did not improve\n",
      "Epoch 39/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0128 - mean_absolute_error: 0.0565 - mean_absolute_percentage_error: 10026750.3872 - val_loss: 0.0201 - val_mean_absolute_error: 0.0696 - val_mean_absolute_percentage_error: 24209179.7470\n",
      "\n",
      "Epoch 00039: val_loss did not improve\n",
      "Epoch 40/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0156 - mean_absolute_error: 0.0606 - mean_absolute_percentage_error: 13260500.1419 - val_loss: 0.0174 - val_mean_absolute_error: 0.0657 - val_mean_absolute_percentage_error: 18360781.4190\n",
      "\n",
      "Epoch 00040: val_loss did not improve\n",
      "Epoch 41/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0116 - mean_absolute_error: 0.0544 - mean_absolute_percentage_error: 9385624.3541 - val_loss: 0.0149 - val_mean_absolute_error: 0.0615 - val_mean_absolute_percentage_error: 10186398.9822\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.01507 to 0.01487, saving model to ./model.h5\n",
      "Epoch 42/1000\n",
      "4044/4044 [==============================] - 0s 69us/step - loss: 0.0113 - mean_absolute_error: 0.0527 - mean_absolute_percentage_error: 8119919.5509 - val_loss: 0.0158 - val_mean_absolute_error: 0.0623 - val_mean_absolute_percentage_error: 15316476.8617\n",
      "\n",
      "Epoch 00042: val_loss did not improve\n",
      "Epoch 43/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0115 - mean_absolute_error: 0.0538 - mean_absolute_percentage_error: 9144707.8655 - val_loss: 0.0153 - val_mean_absolute_error: 0.0612 - val_mean_absolute_percentage_error: 9559146.2806\n",
      "\n",
      "Epoch 00043: val_loss did not improve\n",
      "Epoch 44/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0141 - mean_absolute_error: 0.0576 - mean_absolute_percentage_error: 11260617.5747 - val_loss: 0.0168 - val_mean_absolute_error: 0.0623 - val_mean_absolute_percentage_error: 9276046.0000\n",
      "\n",
      "Epoch 00044: val_loss did not improve\n",
      "Epoch 45/1000\n",
      "4044/4044 [==============================] - 0s 62us/step - loss: 0.0178 - mean_absolute_error: 0.0621 - mean_absolute_percentage_error: 12461306.2203 - val_loss: 0.0176 - val_mean_absolute_error: 0.0630 - val_mean_absolute_percentage_error: 10576103.7510\n",
      "\n",
      "Epoch 00045: val_loss did not improve\n",
      "Epoch 46/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0124 - mean_absolute_error: 0.0553 - mean_absolute_percentage_error: 10584551.6396 - val_loss: 0.0147 - val_mean_absolute_error: 0.0588 - val_mean_absolute_percentage_error: 8365706.1008\n",
      "\n",
      "Epoch 00046: val_loss improved from 0.01487 to 0.01474, saving model to ./model.h5\n",
      "Epoch 47/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0114 - mean_absolute_error: 0.0528 - mean_absolute_percentage_error: 8365074.4946 - val_loss: 0.0145 - val_mean_absolute_error: 0.0576 - val_mean_absolute_percentage_error: 9773233.3409\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.01474 to 0.01454, saving model to ./model.h5\n",
      "Epoch 48/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0109 - mean_absolute_error: 0.0520 - mean_absolute_percentage_error: 7987958.9261 - val_loss: 0.0147 - val_mean_absolute_error: 0.0597 - val_mean_absolute_percentage_error: 11758724.9585\n",
      "\n",
      "Epoch 00048: val_loss did not improve\n",
      "Epoch 49/1000\n",
      "4044/4044 [==============================] - 0s 45us/step - loss: 0.0104 - mean_absolute_error: 0.0508 - mean_absolute_percentage_error: 7343329.6795 - val_loss: 0.0151 - val_mean_absolute_error: 0.0607 - val_mean_absolute_percentage_error: 10560124.3676\n",
      "\n",
      "Epoch 00049: val_loss did not improve\n",
      "Epoch 50/1000\n",
      "4044/4044 [==============================] - 0s 45us/step - loss: 0.0100 - mean_absolute_error: 0.0497 - mean_absolute_percentage_error: 6493144.3380 - val_loss: 0.0170 - val_mean_absolute_error: 0.0623 - val_mean_absolute_percentage_error: 17517120.3597\n",
      "\n",
      "Epoch 00050: val_loss did not improve\n",
      "Epoch 51/1000\n",
      "4044/4044 [==============================] - 0s 46us/step - loss: 0.0117 - mean_absolute_error: 0.0526 - mean_absolute_percentage_error: 8951683.0089 - val_loss: 0.0149 - val_mean_absolute_error: 0.0605 - val_mean_absolute_percentage_error: 10341413.5731\n",
      "\n",
      "Epoch 00051: val_loss did not improve\n",
      "Epoch 52/1000\n",
      "4044/4044 [==============================] - 0s 45us/step - loss: 0.0119 - mean_absolute_error: 0.0536 - mean_absolute_percentage_error: 8389135.4777 - val_loss: 0.0148 - val_mean_absolute_error: 0.0599 - val_mean_absolute_percentage_error: 10421095.5020\n",
      "\n",
      "Epoch 00052: val_loss did not improve\n",
      "Epoch 53/1000\n",
      "4044/4044 [==============================] - 0s 47us/step - loss: 0.0103 - mean_absolute_error: 0.0502 - mean_absolute_percentage_error: 6777937.6499 - val_loss: 0.0140 - val_mean_absolute_error: 0.0567 - val_mean_absolute_percentage_error: 10149983.2510\n",
      "\n",
      "Epoch 00053: val_loss improved from 0.01454 to 0.01401, saving model to ./model.h5\n",
      "Epoch 54/1000\n",
      "4044/4044 [==============================] - 0s 57us/step - loss: 0.0116 - mean_absolute_error: 0.0520 - mean_absolute_percentage_error: 8272144.6756 - val_loss: 0.0147 - val_mean_absolute_error: 0.0564 - val_mean_absolute_percentage_error: 7223743.8355\n",
      "\n",
      "Epoch 00054: val_loss did not improve\n",
      "Epoch 55/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0108 - mean_absolute_error: 0.0511 - mean_absolute_percentage_error: 7398129.0111 - val_loss: 0.0158 - val_mean_absolute_error: 0.0581 - val_mean_absolute_percentage_error: 13271988.1957\n",
      "\n",
      "Epoch 00055: val_loss did not improve\n",
      "Epoch 56/1000\n",
      "4044/4044 [==============================] - 0s 48us/step - loss: 0.0108 - mean_absolute_error: 0.0504 - mean_absolute_percentage_error: 7011504.1775 - val_loss: 0.0146 - val_mean_absolute_error: 0.0592 - val_mean_absolute_percentage_error: 12277425.0929\n",
      "\n",
      "Epoch 00056: val_loss did not improve\n",
      "Epoch 57/1000\n",
      "4044/4044 [==============================] - 0s 48us/step - loss: 0.0097 - mean_absolute_error: 0.0490 - mean_absolute_percentage_error: 6427642.2054 - val_loss: 0.0163 - val_mean_absolute_error: 0.0578 - val_mean_absolute_percentage_error: 5787960.0662\n",
      "\n",
      "Epoch 00057: val_loss did not improve\n",
      "Epoch 58/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0114 - mean_absolute_error: 0.0513 - mean_absolute_percentage_error: 7908703.1492 - val_loss: 0.0138 - val_mean_absolute_error: 0.0569 - val_mean_absolute_percentage_error: 9234687.6759\n",
      "\n",
      "Epoch 00058: val_loss improved from 0.01401 to 0.01380, saving model to ./model.h5\n",
      "Epoch 59/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0111 - mean_absolute_error: 0.0509 - mean_absolute_percentage_error: 7426393.1165 - val_loss: 0.0186 - val_mean_absolute_error: 0.0606 - val_mean_absolute_percentage_error: 5989207.6897\n",
      "\n",
      "Epoch 00059: val_loss did not improve\n",
      "Epoch 60/1000\n",
      "4044/4044 [==============================] - 0s 58us/step - loss: 0.0120 - mean_absolute_error: 0.0526 - mean_absolute_percentage_error: 8175672.3492 - val_loss: 0.0145 - val_mean_absolute_error: 0.0577 - val_mean_absolute_percentage_error: 8364934.9941\n",
      "\n",
      "Epoch 00060: val_loss did not improve\n",
      "Epoch 61/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0101 - mean_absolute_error: 0.0496 - mean_absolute_percentage_error: 6739239.6233 - val_loss: 0.0147 - val_mean_absolute_error: 0.0566 - val_mean_absolute_percentage_error: 7648247.9190\n",
      "\n",
      "Epoch 00061: val_loss did not improve\n",
      "Epoch 62/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0135 - mean_absolute_error: 0.0550 - mean_absolute_percentage_error: 9781504.4748 - val_loss: 0.0225 - val_mean_absolute_error: 0.0663 - val_mean_absolute_percentage_error: 5307091.5823\n",
      "\n",
      "Epoch 00062: val_loss did not improve\n",
      "Epoch 63/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0150 - mean_absolute_error: 0.0570 - mean_absolute_percentage_error: 10524379.8892 - val_loss: 0.0154 - val_mean_absolute_error: 0.0599 - val_mean_absolute_percentage_error: 10860121.4328\n",
      "\n",
      "Epoch 00063: val_loss did not improve\n",
      "Epoch 64/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0130 - mean_absolute_error: 0.0527 - mean_absolute_percentage_error: 7109370.0188 - val_loss: 0.0270 - val_mean_absolute_error: 0.0792 - val_mean_absolute_percentage_error: 34556976.2846\n",
      "\n",
      "Epoch 00064: val_loss did not improve\n",
      "Epoch 65/1000\n",
      "4044/4044 [==============================] - 0s 52us/step - loss: 0.0229 - mean_absolute_error: 0.0712 - mean_absolute_percentage_error: 19614413.4065 - val_loss: 0.0199 - val_mean_absolute_error: 0.0750 - val_mean_absolute_percentage_error: 20823096.5257\n",
      "\n",
      "Epoch 00065: val_loss did not improve\n",
      "Epoch 66/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0117 - mean_absolute_error: 0.0554 - mean_absolute_percentage_error: 10431739.2065 - val_loss: 0.0135 - val_mean_absolute_error: 0.0572 - val_mean_absolute_percentage_error: 10054558.9012\n",
      "\n",
      "Epoch 00066: val_loss improved from 0.01380 to 0.01352, saving model to ./model.h5\n",
      "Epoch 67/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0099 - mean_absolute_error: 0.0502 - mean_absolute_percentage_error: 7365344.5806 - val_loss: 0.0136 - val_mean_absolute_error: 0.0557 - val_mean_absolute_percentage_error: 7516630.5415\n",
      "\n",
      "Epoch 00067: val_loss did not improve\n",
      "Epoch 68/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0101 - mean_absolute_error: 0.0494 - mean_absolute_percentage_error: 6860212.8971 - val_loss: 0.0151 - val_mean_absolute_error: 0.0568 - val_mean_absolute_percentage_error: 6825793.4555\n",
      "\n",
      "Epoch 00068: val_loss did not improve\n",
      "Epoch 69/1000\n",
      "4044/4044 [==============================] - 0s 51us/step - loss: 0.0107 - mean_absolute_error: 0.0498 - mean_absolute_percentage_error: 7087004.8853 - val_loss: 0.0140 - val_mean_absolute_error: 0.0566 - val_mean_absolute_percentage_error: 6265195.3226\n",
      "\n",
      "Epoch 00069: val_loss did not improve\n",
      "Epoch 70/1000\n",
      "4044/4044 [==============================] - 0s 54us/step - loss: 0.0116 - mean_absolute_error: 0.0520 - mean_absolute_percentage_error: 7495335.5522 - val_loss: 0.0205 - val_mean_absolute_error: 0.0682 - val_mean_absolute_percentage_error: 21788934.2332\n",
      "\n",
      "Epoch 00070: val_loss did not improve\n",
      "Epoch 71/1000\n",
      "4044/4044 [==============================] - 0s 47us/step - loss: 0.0136 - mean_absolute_error: 0.0548 - mean_absolute_percentage_error: 9743292.9465 - val_loss: 0.0177 - val_mean_absolute_error: 0.0625 - val_mean_absolute_percentage_error: 16546445.9941\n",
      "\n",
      "Epoch 00071: val_loss did not improve\n",
      "Epoch 72/1000\n",
      "4044/4044 [==============================] - 0s 50us/step - loss: 0.0107 - mean_absolute_error: 0.0504 - mean_absolute_percentage_error: 7412803.8190 - val_loss: 0.0142 - val_mean_absolute_error: 0.0555 - val_mean_absolute_percentage_error: 9847454.8053\n",
      "\n",
      "Epoch 00072: val_loss did not improve\n",
      "Epoch 73/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0095 - mean_absolute_error: 0.0484 - mean_absolute_percentage_error: 6130538.7859 - val_loss: 0.0146 - val_mean_absolute_error: 0.0583 - val_mean_absolute_percentage_error: 6164745.9550\n",
      "\n",
      "Epoch 00073: val_loss did not improve\n",
      "Epoch 74/1000\n",
      "4044/4044 [==============================] - 0s 56us/step - loss: 0.0091 - mean_absolute_error: 0.0466 - mean_absolute_percentage_error: 5186480.6845 - val_loss: 0.0141 - val_mean_absolute_error: 0.0580 - val_mean_absolute_percentage_error: 10223022.6443\n",
      "\n",
      "Epoch 00074: val_loss did not improve\n",
      "Epoch 75/1000\n",
      "4044/4044 [==============================] - 0s 48us/step - loss: 0.0097 - mean_absolute_error: 0.0485 - mean_absolute_percentage_error: 5744484.4520 - val_loss: 0.0147 - val_mean_absolute_error: 0.0583 - val_mean_absolute_percentage_error: 11693968.5929\n",
      "\n",
      "Epoch 00075: val_loss did not improve\n",
      "Epoch 76/1000\n",
      "4044/4044 [==============================] - 0s 53us/step - loss: 0.0093 - mean_absolute_error: 0.0474 - mean_absolute_percentage_error: 5812903.8215 - val_loss: 0.0137 - val_mean_absolute_error: 0.0563 - val_mean_absolute_percentage_error: 9358128.5257\n",
      "\n",
      "Epoch 00076: val_loss did not improve\n",
      "Epoch 00076: early stopping\n",
      "[INFO] Best loss: 0.009123359743273577\n",
      "[INFO] Best val_loss: 0.013524363005143616\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 建構 model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, input_shape=(X_train.shape[1],), activation=\"relu\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dense(32, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.05))\n",
    "model.add(layers.Dense(16, activation=\"relu\"))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer=\"adam\", metrics=['mae','MAPE'])\n",
    "\n",
    "# 顯示模型摘要與結構\n",
    "model.summary()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=10, mode='auto', verbose=1)\n",
    "checkpointer = ModelCheckpoint('./model.h5',verbose=1, save_best_only=True)\n",
    "\n",
    "# 開始訓練 model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=1000, batch_size=128, callbacks=[es,checkpointer])\n",
    "\n",
    "print(\"[INFO] Best loss: {}\".format(np.min(history.history['loss'])))\n",
    "print(\"[INFO] Best val_loss: {}\".format(np.min(history.history['val_loss'])))\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Model Performance')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1265/1265 [==============================] - 0s 71us/step\n",
      "Test MAPE : 7.299486594866207\n",
      "Test RMSE : 0.10880870090189568\n",
      "Test MAE : 0.052451139918193515\n"
     ]
    }
   ],
   "source": [
    "# 評估指標\n",
    "mse, mae, mape = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Test MAPE : \" + str(mean_absolute_percentage_error(y_test , y_pred)))\n",
    "print(\"Test RMSE : \" + str(np.sqrt(mse)))\n",
    "print(\"Test MAE : \" + str(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python36(opencv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
